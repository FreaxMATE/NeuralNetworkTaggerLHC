# Neural Network Tagger for Hadronic Vâ†’qqÌ„ Jets

A compact ML pipeline that trains a neural network to tag boosted W/Zâ†’qqÌ„ jets and compares it to a cut-based baseline, then applies the model to ATLAS Open Data.

Learn more about the data source: https://opendata.atlas.cern/

## ğŸŒŸ Overview

This project provides an end-to-end workflow to:
- Load MC and data from CSVs (`pythia.csv`, `jets.csv`)
- Build features from event objects (leading jet/lepton kinematics and angular variables)
- Train a small MLP classifier (PyTorch)
- Evaluate training/validation curves, ROC and purity
- Choose a working point and apply to real data

Outputs (plots, CSVs, and model weights) are written under `out/<revision>/`.

## ğŸ“ Project Structure

```
NeuralNetworkTaggerLHC/
â”œâ”€â”€ data-exercise-template-v7.py   # Main script: data loading, MLP training, evaluation, plots
â”œâ”€â”€ jets.csv                       # ATLAS Open Data (flattened events)
â”œâ”€â”€ pythia.csv                     # MC in the same format (+ truth label for jets)
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ setup.sh                       # Quick setup script (Linux/macOS)
â”œâ”€â”€ setup.bat                      # Quick setup script (Windows)
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ flake.nix                      # Nix development environment (optional)
â”œâ”€â”€ flake.lock                     # Nix lockfile
â”œâ”€â”€ aux/                           # Optional: generators and skimming utilities
â”‚   â”œâ”€â”€ main213.cc
â”‚   â””â”€â”€ skimevents.py
â””â”€â”€ report/
    â””â”€â”€ report.md                  # Notes/report template (optional)
```

## ğŸš€ Features

- Lightweight event model (`Particle`, `Event`) and CSV loader
- Cut-based selection (baseline) with configurable kinematic cuts
- MLP classifier in PyTorch with train/val split and scheduler
- Metrics and plots:
  - Training vs validation loss and accuracy
  - Score distribution and chosen threshold t*
  - ROC curve (manual AUC)
  - Purity vs threshold and efficiencies
- Application to real data (`jets.csv`) with before/after selection mass histograms

## ğŸ› ï¸ Dependencies

Python packages used by the main script:
- numpy
- matplotlib
- scikit-learn
- torch (PyTorch)

An optional Nix flake is provided for fully reproducible dev shells.

## ğŸ’» Setup & Installation

### Option 1: Quick Setup (Recommended)

Linux/macOS:
```bash
# Run the setup script (uses Python venv and pip)
bash ./setup.sh
```

Windows:
```bat
:: Run the setup script (creates venv and installs requirements)
setup.bat
```

After setup, activate the environment:
- Bash/zsh: `source .venv/bin/activate`
- Fish: `source .venv/bin/activate.fish`
- Windows (cmd): `.venv\Scripts\activate`

### Option 2: Manual Setup (pip)

```bash
python -m venv .venv
# Bash/zsh:  source .venv/bin/activate
# Fish:      source .venv/bin/activate.fish
pip install -r requirements.txt
```

If PyTorch fails to install from PyPI on your platform, consult https://pytorch.org for the appropriate install command (CPU/GPU, CUDA version), or use the Nix or conda options below.

### Option 3: Conda (optional)

```bash
conda create -n nn-tagger python=3.12
conda activate nn-tagger
pip install -r requirements.txt
```

### Option 4: Nix (Advanced/Optional)

The repo includes a Nix flake that provides a development shell with all core Python deps:

```bash
nix develop
```

## ğŸ¯ Usage

1) Ensure your environment is activated (venv, conda, or nix shell).
2) Run the main script:

```bash
python data-exercise-template-v7.py
```

Key configurable settings are defined near the top of the script:
- Input files: `MC_FILE`, `DATA_FILE`
- Baseline cuts: `min_pt_j`, `min_pt_l`, `min_dphi`, `eta_j_max`
- Training: `epochs`, `batch_size`, optimizer/scheduler
- Output revision tag: `revision` (controls `out/<rev>/` directory)

## ğŸ“Š Output

Artifacts are saved to `out/<revision>/`, including:
- `<title>_model.pt` â€” trained model weights
- `<title>.png` â€” training curves (loss/accuracy/LR)
- `<title>_scores.png` â€” test score distribution with t*
- `<title>_roc.png` â€” ROC plot (manual AUC)
- `<title>_purity.png` â€” purity vs threshold with working point
- `<title>_jetmass.png` â€” data mass histogram before/after cuts
- CSVs: training history, ROC points, purity curve (if writing succeeds)

## ğŸ“ˆ Results (sample outputs)

Below are example figures produced by `data-exercise-template-v7.py` (see also `report/report.md`). Paths assume the default `revision='17'`:

![Training: loss, accuracy, learning rate](out/17/training_v17.png)

![ROC curve on MC test split](out/17/training_v17_roc.png)

![Purity vs threshold with working point t*](out/17/training_v17_purity.png)

![Classifier scores on MC test split](out/17/training_v17_scores.png)

![Data: jet mass before/after cuts](out/17/training_v17_jetmass.png)

If these images are not present yet, run the script to generate them; they will be written under `out/<revision>/`.

## ğŸ§ª Data format (CSV)

Each row contains a single reconstructed particle. Events are grouped by `event_id` and split into large-R jets (pid=Â±90) and leptons (others). For each event, the script builds a feature vector from the leading jet and leading lepton.

## âš™ï¸ Tips & Notes

- Reproducibility: seeds for `random`, `numpy`, and `torch` are set to 42.
- GPU is optional; the model is tiny and trains fast on CPU. For GPU, install the appropriate CUDA-enabled PyTorch wheel.
- If you change the feature set, adjust the network input dimension accordingly.

### Troubleshooting

- Linux: if `ImportError: libstdc++.so.6` appears when importing NumPy, install your system's C++ runtime (e.g., `sudo apt install libstdc++6`). Using the provided Nix flake also avoids these issues.
- Fish shell: activate with `source .venv/bin/activate.fish`.
- Force CPU-only torch wheel on Linux: run `TORCH_CPU=1 bash setup.sh`.

## ğŸ“„ License

This project is licensed under the terms in the `LICENSE` file.

## ğŸ¤ Acknowledgments

Built for educational purposes using ATLAS Open Data. Thanks to the open data community for resources and inspiration.

---

*Part of Applied Computational Physics and Machine Learning coursework*